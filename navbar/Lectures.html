<section id="main_content" class="inner" style="height:100%;display:block;">
    <style>
        style_noborder{
        border-width: 0px;
        }
        .style_white {
        border-style: solid;
        border-width: 1px;
        background-color: #ffffff;
             }
     .style_emphasize {
        border-style: solid;
        border-width: 1px;
        background-color:#F4F2D8;;
             }
     </style>
    <table style="width: 100%" class="style_noborder ">
        <tr>
            <td class="style_emphasize " colspan="3"> 
                <ul style="color: #cc0000"> 
                    <li>Lectures(first 2 weeks):
                        <ul>
                            <li>Conducted through <a href="https://meet.google.com/pcn-qgqp-hfv">Lectures-Google meet</a></li>
                        </ul>
                    </li>
                    <li>Office hours:
                        <ul>
                            <li>Conducted through  <a href="https://meet.google.com/mgu-bodg-xhd?pli=1&authuser=1">Office Hours-Google Meet </a></li>

                            <li>MWF 10:15-11:15am</li>
						<li>T  11:10am-1:00pm</li>
						<li>Th 11:10am-12:20pm</li>
                            <li>Or drop me an <a href = "mailto: keith.perkins@cnu.edu">Email</a> and we can set up a something that works for you</li>
                        </ul>
                    </li> 
                    <li> Textbooks: None required, but I find the following helpful
                        <ul>
                            <li>'Python for Data Analysis' by Wes McKinney - mostly for pandas help</li>
                            <li>'Machine Learning with Python Cookbook' by Chris Albon - short data/ml recipes</li>
                            <li>'Python Machine Learning' by Sebastian Raschka - Excellent machine learning introduction</li>
                        </ul>
                    </li>
                  </ul>
            </td>
    
        </tr>
         <tr>
            <td class="style_white "><em><strong>Week</strong></em></td>
            <td class="style_white "><em><strong>Lectures/Readings</strong></em></td>
            <td class="style_white "><em><strong>Examples</strong></em></td>
        </tr>
        <tr>
            <td class="style_white "><em>1</em></td>
            <td class="style_white ">
                <!--<ul><strong>Tools to help you keep track of research</strong>
                    <li>A note taking App (like evernote)</li>
                    <li>A web highlighter, to highlight and web pages (like Weava)</li>
                    <li><a href="arxiv.org">ArXiv</a> - pre print of technical papers, go here to read original research</li>
                    <li>A place to store documents you are reading (like <a href="https://www.mendeley.com">Mendeley</a>)</li>
                
                    <li>Jupyter extensions for Jupyter notebooks (these can be finniky)</li>
                </ul>-->
                <strong>Lectures</strong><BR>
                <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week1/1_Introduction.pdf"> Syllabus and Course Introduction</a></strong><br>
                <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week1/2_workflow.pdf"> A general approach to a Data Science question</a></strong> there are lots of variations<br>
                <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week1/3_tools.pdf"> Anaconda, Virtual Environments, Jupyter Lab, Optional record keeping tools</a></strong> Tools for this class and some optional tools to help you keep track of information<br>
                <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week1/3_setup_data301_python_environment_with_notebook_templates.txt"> List of commands to set up a virtual environment, switch to it and then invoke jupyter lab</a></strong>  Plus a bit on configuring notebook templates<br><br>
                <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/hotkeys/hotkeys.txt" style="color: #00cc00"> Jupyter Lab shortcuts Keys</a></strong><br>
                Jupyter notebooks are a great way to prototype; you can write a bit of code, test it and change it without having to restart a python kernel.  
                This is a huge debugging speedup since you only load data once, you do not have to reload it every time you change your code.<br>
                On the other hand, the debugger is primitive when compared to a full fledged IDE like PyCharm.<br>
                Also jupyter notebooks are used to prototype your solution.  Once everything works, you'll probably export the salient bits to packages and scripts to be used by other components, like a webservice or dashboard.<br>
                What's a dashboard? Its a way to aggregate and interactively display data in a user friendly way.  Here is an example <a href="https://kp-avocado.herokuapp.com">dashboard</a> (takes a bit to provision and load) created from <a href="https://realpython.com/python-dash/">this</a> excellent tutorial.
                  <br><br>
              
                <strong>References</strong><BR>
                <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week1/Pandas_Cheat_Sheet.pdf"> Pandas cheatsheet</a></strong><br>                   
                <strong><a href="https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf"> Anaconda cheatsheet</a></strong><br>
                <strong><a href="https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/">Virtual Environments</a></strong><br>
                <br>
                Anaconda and Windows - getting Jupyter Lab up and running in your virtual environment of choice (one of several ways)<BR>
                    <ol>
                        <li>Install anaconda</li>
                        <li>Run 'Anaconda Prompt'(use Windows search to find it)</li>
                        <li>Create virtual environment and activate it</li>
                        <li>start jupyter lab</li>
                    </ol>
                    A jupyter Lab browser window will appear
                <!--review 201 pandas, open, close, save (feather format?). <br>
                Statistics (nunique, describe, is_null(), isnull().sum()), info (dtype), indexes (reset_index()),  <br>
                Selection (slices, using boolean index, by column, by row), grouping (group_by)<br>
                sampling (if dataset is large get a subset, make sure subset has same mean and std as original)<br><br>
                New:<br>
                something new, joining (very similar to databases inner and outter join)<br>
                seaborn (fewer lines to plot)<br>
                plotly (interactive plots)<br><br>


                missing data- find it, average? closest (how?), predict with a model?<br>
                create artificial dataset with height weight, tshirt sizes.  Delete 10% of sizes, fit linear regression on remaining sizes, predict missing<br>
                show how median just fills in the average, show how linear regression is a little smarter<br>
                use gaussian distribution for weights verses sizes.<br> 

                Built in to sklearn<a href="https://scikit-learn.org/stable/modules/impute.html">Imputation of missing values</a> 
                and <a href="https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py">Imputing missing values before building an estimator</a><br>-->

            </td>
            <td class="style_white ">
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_1/1_JupyterUsage.ipynb">How to efficiently navigate a Jupyter Notebook</a></strong> Some keyboard shortcuts, "Magic" commands, how to run shell commands, getting API help, and an introduction to debugging in a Jupyter Notebook<br>
            </td>
        </tr>
        <td class="style_white "><em>2</em></td>
            <td class="style_white ">
                MLK Holiday <br>
                <strong>References</strong><BR>
                    
                <a href="https://www.kaggle.com/alexisbcook/inconsistent-data-entry"> Kaggle- text pre processing</a>Very good, especially the fuzzywuzzy module<br>              
                <a href="https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing">Text pre-processing</a> Good, although some of it (spell checking for instance) may not scale well<br>
                <a href=" https://realpython.com/inner-functions-what-are-they-good-for/#retaining-state-with-inner-functions-closures "> Real Python- Closures</a>  Closures are functions that have state, they are a good way to 
                genericize a function that you are applying to a DataFrame column in pandas.  See the 'Cleaning Strings' notebook below for an example.<br>
                <a href=" https://realpython.com/regex-python-part-2/#substitution-functions"> Real Python- Regular Expressions - re.sub</a><br> 
               <!-- <a href=" https://realpython.com/regex-python/ "> Regular Expressions- part 1</a><br> -->
                 <!-- <a href=" https://realpython.com/regex-python-part-2/ "> Regular Expressions- part 2</a><br> -->
                                 
            </td>
            <td class="style_white ">
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_2/22_review_chapter_5.ipynb">Pandas Intro</a></strong> Chapter 5, McKinney<br>
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_2/23_review_chapter_6.ipynb">Data Loading and Saving</a></strong> Chapter 6, McKinney<br>  
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_2/24_cleaning_strings.ipynb">Cleaning strings</a></strong> Some introductory string cleaning suggestions demonstrated by normalizing user entries for 'Country'. Starts with standard transforms and finishes with task specific transforms using <a href="https://p
                ypi.org/project/pycountry/">pycountry</a>. (BTW check out the nifty tqdm progress meter, very useful for long running bits of code) <br>  
                

            </td>
        </tr>
        <td class="style_white "><em>3</em></td>
            <td class="style_white ">
                <strong>Lectures</strong><BR>
                    <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week3/31_data_cleaning.pdf"> Cleaning Data </a></strong> Get rid of duplicates and NaNs, organize free form text. Further, data used by Machine Learning algorithms must be numeric,
                    so add in categorization and possible one hot encoding, scaling and normalizing, date/time formatting, dimensionality reduction and binning if needed/wanted.  There are others but these are enough to get started.<br>
                  
                 <strong>References</strong><BR>
                    Chapter 7 in McKinney <br>
                    Chapter 10, up to section 10.3, 'Suppressing the Group Keys'<br><br>
                    
                    <!-- https://www.statology.org/cluster-analysis-real-life-examples/ -->
                <!-- <a href="https://towardsdatascience.com/how-to-structure-your-data-science-workflow-b06748b7761a>EXCELLENT- Full exploration and solution using titanic dataset,exploration is great, see the catplot for male/female survival</a> -->
                <strong>Homework</strong><BR>
                    See if you can replace map with apply in the <a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_2/24_cleaning_strings.ipynb">Cleaning Strings</a></strong> notebook<br> section 1.13,"Add a new column to track the changed columns"<br> 
           </td>
            <td class="style_white ">
                     <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_3/31_cleaning_missing_and_duplicate_data.ipynb">Handling missing and duplicate data</a></strong> Duplicates are easy, just delete them. Missing data is harder. 
                     (BTW check out the nifty <a href="https://pypi.org/project/names/https://pypi.org/project/names/">names </a> module, for generating random names)<br>  
                     <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_3/32_categorical_variables.ipynb"> Handling categorical data</a></strong>How to handle both nominal and ordinal categorical data<br>
                    
                     <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_3/33_groupby_apply.ipynb">Groupby and apply</a></strong> Powerful data query APIs used to explore the titanic dataset<br>        
            
            </td>
        </tr>
        <td class="style_white "><em>4</em></td>
            <td class="style_white ">
                <strong>Lectures</strong><BR>
                    <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week4/41_create_an_importable_package.pdf" style="color: #00cc00">Turn utils.py into a package</a></strong> utils.py is a module which can be accessed directly from python files in the same folder. In order to get to it from other folders it must be converted to a package, here is a simple way to do that.<br>
                    
                 <br><strong>References</strong><BR>
                    <strong><a href="https://towardsdatascience.com/understanding-python-imports-init-py-and-pythonpath-once-and-for-all-4c5249ab6355 ">Python Packages tutorial</a></strong>Does not include namespace packages<br>
                    <strong><a href="https://www.kdnuggets.com/2021/01/cleaner-data-analysis-pandas-pipes.html">Pipelines- a way to iteratively apply a list of functions to a dataframe</a></strong>An excellent way to simplify and streamline data pre processing.<br>
                    <strong><a href="https://www.youtube.com/watch?v=FgakZw6K1QQ">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a></strong>For those of you who want more information on PCA<br>
                    <!-- https://www.youtube.com/watch?v=oRvgq966yZg -- if fewer samples than variables then the number of samples puts an upper bound on the number of PCs -->
                    <strong><a href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py">Importance of Feature Scaling</a></strong><br>
                    <!-- <strong><a href="https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"> Scaling data</a> -->
                     
                    <!--
                    https://towardsdatascience.com/autoencoders-vs-pca-when-to-use-which-73de063f5d7 Autoencoders verses PCA- OK article, essentially, use autoencoder when features have non linear relationship with each other, has OK examples
                    https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29-->
                <br><strong>Homework</strong><BR>
                    1. Move the functions from 'Set up Transforms' section of <a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/43_pipeline_complete_preprocess.ipynb">A data pre-processing Pipeline</a> 
                    notebook into a seperate package, import and use these functions in the notebook<br> 
                    2. In the <a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/45_dimenionality_reduction.ipynb">Feature Reduction</a> notebook, fill in t-SNE section, <mark>CAUTION: t-SNE is used just for visualization, not for dimensionality reduction since it does not preserve distance info.</mark><br>
                    3. Time all 3 methods, PCA, t-SNE, and UMAP.  (Use %%time cell method) to get relative performance information specific to this problem.  BTW You should always consider algorithm efficiency (In computer science we use Big O notation as a proxy for this).  
                    But first and foremost you want the correct answer, PCA will get the same answer every time, while t-SNE will not given it's probablistic nature and potential to get stuck in local minima.  In other words, you likely get different answers on different runs for t-SNE (it may also find structure where none exists). OTOH, it generates pretty clusters.<br>
       
            </td>
            <td class="style_white ">
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/42_Processing_Numerical_data.ipynb"> Scaling Numerical Data</a></strong> After all categorical variables have been converted to numerical data, scale it with these transforms. (Note: this is required in order to use this data as input to many ML algorithms)<br><br>
                    

                            
                These 2 notebooks demonstrate; using the package created in the left pane and pipelines.  Both are useful when porting a notebook to a script. 
                <br>
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/41_loading_util_package_from another_folder.ipynb" style="color: #00cc00"> Using the new utils package</a></strong>How to add the proper path to the package, how to browse the package contents<br>
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/43_pipeline_complete_preprocess.ipynb">A data pre-processing Pipeline</a></strong> A way to iteratively apply a list of transforms to a dataframe.  Pipelines streamline pre-processing <br>
                <!-- a script which does all the data preprocessing steps shown in the jupyter notebook -->
                <br><br>Jupyter notebooks are for rapid prototyping. They make it easy to load data once, and then iteratively and quickly run and document experiments. 
                They are not meant for production environments since they tend to display lots of UI and require intervention to run each cell.  
                If your notebook is purely exploratory, that is it only plots various aspects of data, then you need do nothing else. 
                If however, your notebook produces something (like a processed dataset), then it's a good idea to port it's relevant content to a script that requires no user interaction, and displays no UI. This approach has the following advantages:<br>
                1. It ensures repeatable results since the script always executes the same way.<br>
                2. The script can be executed by another script, which means it can be a building block used in other processes.<br><br>
                
                <strong>General steps to port a jupyter notebook to a python script:</strong><br>
                <!--discussed nbconvert and its drawbacks (exports all code when you only want non-ui code).  Before doing any of this write on the board-->
                <ol>
                <li>Ignore all notebook code that explores the data (plotting with matplotlib, seaborn, various DataFrame statistics like unique and value_counts)</li>
                <li>Export relevant functions to a python file (idealy in a seperate package for maximum flexibility).</li>
                <li>Create a guarded section in the python file that runs only if it is executed as a script, not when it's imported as a module.  It will start with 'if __name__== "__main__":'</li>
                <li>Place all code that should run in this guarded section.</li>
                </ol>
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/preprocess_demo.py">A data pre-processing Pipeline script</a></strong>The converted script version of the above notebook, 'A data pre-processing Pipeline' <br>
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/44_pandas_merges.ipynb">Pandas merges</a></strong>A segue: In case you are struggling with merging the salary dataset and your currency dataset, here is an overview of the different merges available<br><br>
                <br>See Albon, recipes 9.1 and 10.3 for PCA and Removing highly correlated features<br>
                <strong><a href="https://github.com/CNUClasses/DATA301_CODE/blob/master/week_4/45_dimenionality_reduction.ipynb">Feature reduction</a></strong>Removing highly correlated features and PCA for feature reduction<br><br>
             </td>
        </tr>
        <tr>
            <td class="style_white "><em>5</em></td>
            <td class="style_white ">
                <strong>Lectures</strong><BR>
                    <strong><a href="https://github.com/CNUClasses/DATA301/blob/master/content/lectures/week5/51_ML_types_clustering_introduction.pdf">Machine Learning Introduction</a></strong>
                     with an emphasis on introducing unsupervised, clustering techniques<br>
                                   
                <br><strong>References</strong><BR>
                    <!-- kmeans and heiarchical intro-https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/?utm_source=blog&utm_medium=beginners-guide-hierarchical-clustering -->
                    <strong><a href="https://www.youtube.com/watch?v=4b5d3muPQmA">StatQuest: K-means clustering</a></strong> Scales well, uses distance between points. Tends to produce roundish clusters<br>
                    <!-- tuning DBSCAN and HDBSCAN and OPTICS http://www.sefidian.com/2020/12/18/how-to-determine-epsilon-and-minpts-parameters-of-dbscan-clustering/ -->
                    <strong><a href="https://www.youtube.com/watch?v=RDZUdRSDOok">StatQuest: DBSCAN</a></strong> Density based clustering.  Scales well, uses distance between <u>nearest</u> points. Can handle non-spherical as well as nested clusters.  Can also be used to identify outliers<br>
                    <!-- <strong><a href="https://www.youtube.com/watch?v=7xHsRkOdVwo&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=37">StatQuest: Heiarchical Clustering</a>  If I cover it<br> -->
                    <strong><a href="https://www.statology.org/cluster-analysis-real-life-examples/"> 5 Examples of Cluster Analysis in Real Life</a></strong> It is also be used to generate new features (cluster membership) in a dataset.  This enhanced dataset may be easier to interpret by subsequent ML algorithms.<br>
                    
                 <br><strong>Homework</strong><BR>
                  
            </td>
            <td class="style_white ">
            </td>
        </tr>



        <!-- <tr>
            <td class="style_white "><em>5</em></td>
            <td class="style_white ">
                <strong>Lectures</strong><BR>                    
                <br><strong>References</strong><BR>
                <br><strong>Homework</strong><BR>                 
            </td>
            <td class="style_white ">
            </td>
        </tr> -->
   
        

       
        </table>
